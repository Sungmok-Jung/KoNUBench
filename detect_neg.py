import json
import random
from pathlib import Path
from collections import Counter

SHORT_AHN = [" ì•ˆ "]
LONG_AHN  = ["ì§€ì•Š", "ì§€ ì•Š", "ì§„ ì•Š", "ì§€ëŠ” ì•Š", "ì§€ë„ ì•Š", "ì§€ì¡°ì°¨ ì•Š", "ì§€ë§Œì€ ì•Š", "ì§€ë§Œë„ ì•Š", "ì§€ ì•„ë‹ˆ", 
             "ì§„ ì•„ë‹ˆ", "ì§€ëŠ” ì•„ë‹ˆ", "ì§€ì¡°ì°¨ ì•„ë‹ˆ", "ì§€ë§Œì€ ì•„ë‹ˆ", "ì§€ë§Œë„ ì•„ë‹ˆ"]
SHORT_MOT = [" ëª» "]
LONG_MOT  = ["ì§€ëª»", "ì§€ ëª»", "ì§„ ëª»", "ì§€ëŠ” ëª»", "ì§€ë„ ëª»", "ì§€ì¡°ì°¨ ëª»", "ì§€ë§Œì€ ëª»", "ì§€ë§Œë„ ëª»"]
MALDA     = ["ì§€ë§", "ì§€ ë§", "ì§„ ë§", "ì§€ëŠ” ë§", "ì§€ë„ ë§", "ì§€ì¡°ì°¨ ë§", "ì§€ë§Œì€ ë§"]

def make_json_csv(start: int, end: int, total: int, corpus_type: str, seed: int):
    random.seed(seed)
    numbers = random.sample(range(start, end + 1), total)
    print("="*50 + corpus_type + "="*50)
    print(f"ğŸ² random seed: {seed}")
    print(f"âœ… picked_numbers: {numbers}")

    output = []

    counts = Counter({"ì•ˆ ë‹¨í˜•":0, "ì•ˆ ì¥í˜•":0, "ëª» ë‹¨í˜•":0, "ëª» ì¥í˜•":0, "ë§ë‹¤ ë¶€ì •":0, "ì–´íœ˜ì ":0})
    #without lexical negation
    counts_nonlex = Counter({"ì•ˆ ë‹¨í˜•":0, "ì•ˆ ì¥í˜•":0, "ëª» ë‹¨í˜•":0, "ëª» ì¥í˜•":0, "ë§ë‹¤ ë¶€ì •":0})

    total_scanned = 0 
    total_scanned_nonlex = 0     #without lexical negation

    for number in numbers:
        file_path = Path(f"/shared/erc/lab08/korean_negation/{corpus_type}_data/{corpus_type}_data_{number}.json")
        if not file_path.exists():
            print(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {file_path}")
            continue

        try:
            data_list = json.loads(file_path.read_text(encoding="utf-8"))
        except json.JSONDecodeError as e:
            print(f"âŒ JSON ì½ê¸° ì‹¤íŒ¨: {file_path} ({e})")
            continue

        # data_listê°€ ë¦¬ìŠ¤íŠ¸ì¸ì§€ í™•ì¸
        if not isinstance(data_list, list):
            continue

        for data in data_list:
            sentence = data.get("sentence", "")
            data_id  = data.get("data_id", "")
            idx      = data.get("idx", "")
            total_scanned += 1

            detect = {
                "file": file_path.name,
                "data_id": data_id,
                "idx": idx,
                "sentence": sentence,
                "ì•ˆ ë‹¨í˜•": any(p in sentence for p in SHORT_AHN),
                "ì•ˆ ì¥í˜•": any(p in sentence for p in LONG_AHN),
                "ëª» ë‹¨í˜•": any(p in sentence for p in SHORT_MOT),
                "ëª» ì¥í˜•": any(p in sentence for p in LONG_MOT),
                "ë§ë‹¤ ë¶€ì •": any(p in sentence for p in MALDA)
            }

            if not any(detect[k] for k in counts.keys()):
                continue

            output.append(detect)


            for k in counts.keys():
                if detect[k]:
                    counts[k] += 1

            #without lexical negation
            if not detect["ì–´íœ˜ì "]:
                total_scanned_nonlex += 1
                for k in counts_nonlex.keys():
                    if detect[k]:
                        counts_nonlex[k] += 1

    # save
    out_path = Path(f"/shared/erc/lab08/korean_negation/{corpus_type}_neg_start{start}_end{end}_{len(numbers)}files_seed{seed}.json")
    out_path.write_text(json.dumps(output, ensure_ascii=False, indent=2), encoding="utf-8")

    kept = len(output)

    overall_ratio = (kept / total_scanned * 100) if total_scanned else 0.0

    print(f"âœ… {corpus_type}: {kept}ê°œì˜ ë¬¸ì¥ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤ â†’ {out_path}")
    print(f"ğŸ“Š {corpus_type} ë‚´ ë¶€ì •ë¬¸ ë¹„ìœ¨(ì „ì²´): {overall_ratio:.2f}%  (ë¶„ëª¨={total_scanned} ë¬¸ì¥)")

    print(f"\nğŸ“Š {corpus_type} ë°ì´í„° ë¶€ì • í‘œí˜„ ë¶„í¬ (ì „ì²´ ë¶„ëª¨={kept} ë¬¸ì¥)")
    for k, v in counts.items():
        pct = (v / kept * 100) if kept else 0.0
        if k == "ì–´íœ˜ì ":
            print(f"  â€¢ {k}: {v}ê°œ, ì „ì²´ ë¹„ìœ¨: {pct:.2f}%")
        else:
            #without lexical negation
            pct_nonlex = (counts_nonlex[k] / total_scanned_nonlex * 100) if total_scanned_nonlex else 0.0
            print(f"  â€¢ {k}: {v}ê°œ, ì „ì²´ ë¹„ìœ¨: {pct:.2f}%, ì–´íœ˜ì  ì œì™¸ ë¹„ìœ¨: {pct_nonlex:.2f}%")

    return {
        "output": output,
        "counts": counts,
        "counts_nonlex": counts_nonlex,
        "scanned": total_scanned,
        "scanned_nonlex": total_scanned_nonlex,
    }

if __name__ == "__main__":
    seed = 2025
    total = 15

    w = make_json_csv(start=1, end=28264, total=total, corpus_type="written", seed=seed)
    print()
    s = make_json_csv(start=1, end=37790, total=total, corpus_type="spoken",  seed=seed)
    print()

    # ì „ì²´ ìš”ì•½
    kept_total = len(w["output"]) + len(s["output"])
    scanned_total = w["scanned"] + s["scanned"]
    overall_ratio_total = (kept_total / scanned_total * 100) if scanned_total else 0.0
    print(f"ğŸ“Š êµ¬ì–´ì²´+ë¬¸ì–´ì²´ ì „ì²´ ë¶€ì •ë¬¸ ë¹„ìœ¨: {overall_ratio_total:.2f}%  (ë¶„ì={kept_total}, ë¶„ëª¨={scanned_total})")

    print("\nğŸ“Š ìœ í˜•ë³„ í•©ì‚° (ì „ì²´ ë¶„ëª¨=íƒì§€ë¬¸ì¥ í•©ê³„, ì–´íœ˜ì  ì œì™¸ ë¶„ëª¨=ì–´íœ˜ì  ì•„ë‹Œ ë¬¸ì¥ í•©ê³„)")
    for k in ["ì•ˆ ë‹¨í˜•", "ì•ˆ ì¥í˜•", "ëª» ë‹¨í˜•", "ëª» ì¥í˜•", "ë§ë‹¤ ë¶€ì •", "ì–´íœ˜ì "]:
        v_total = w["counts"][k] + s["counts"][k]
        pct_total = (v_total / kept_total * 100) if kept_total else 0.0

        if k == "ì–´íœ˜ì ":
            print(f"  â€¢ {k}: {v_total}ê°œ, ì „ì²´ ë¹„ìœ¨: {pct_total:.2f}%")
        else:
            nonlex_total_den = w["scanned_nonlex"] + s["scanned_nonlex"]
            #without lexical negation
            v_nonlex = w["counts_nonlex"][k] + s["counts_nonlex"][k]
            pct_nonlex_total = (v_nonlex / nonlex_total_den * 100) if nonlex_total_den else 0.0
            print(f"  â€¢ {k}: {v_total}ê°œ, ì „ì²´ ë¹„ìœ¨: {pct_total:.2f}%, ì–´íœ˜ì  ì œì™¸ ë¹„ìœ¨: {pct_nonlex_total:.2f}%")